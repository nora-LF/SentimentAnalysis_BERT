{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>611857364396965889</td>\n",
       "      <td>@aandraous @britishmuseum @AndrewsAntonio Merc...</td>\n",
       "      <td>nocode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>614484565059596288</td>\n",
       "      <td>Dorian Gray with Rainbow Scarf #LoveWins (from...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>614746522043973632</td>\n",
       "      <td>@SelectShowcase @Tate_StIves ... Replace with ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>614877582664835073</td>\n",
       "      <td>@Sofabsports thank you for following me back. ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>611932373039644672</td>\n",
       "      <td>@britishmuseum @TudorHistory What a beautiful ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetid                                               text  \\\n",
       "0  611857364396965889  @aandraous @britishmuseum @AndrewsAntonio Merc...   \n",
       "1  614484565059596288  Dorian Gray with Rainbow Scarf #LoveWins (from...   \n",
       "2  614746522043973632  @SelectShowcase @Tate_StIves ... Replace with ...   \n",
       "3  614877582664835073  @Sofabsports thank you for following me back. ...   \n",
       "4  611932373039644672  @britishmuseum @TudorHistory What a beautiful ...   \n",
       "\n",
       "  emotions  \n",
       "0   nocode  \n",
       "1    happy  \n",
       "2    happy  \n",
       "3    happy  \n",
       "4    happy  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('smile-annotations-final.csv', header=None,\n",
    "                names=['tweetid', 'text', 'emotions'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3085, 3085)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see if tweet ids are unique\n",
    "len(df['tweetid'].unique()), len(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index\n",
    "df.set_index('tweetid', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nocode               1572\n",
      "happy                1137\n",
      "not-relevant          214\n",
      "angry                  57\n",
      "surprise               35\n",
      "sad                    32\n",
      "happy|surprise         11\n",
      "happy|sad               9\n",
      "disgust|angry           7\n",
      "disgust                 6\n",
      "sad|angry               2\n",
      "sad|disgust             2\n",
      "sad|disgust|angry       1\n",
      "Name: emotions, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Emotions')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df.emotions.value_counts())\n",
    "df['emotions'].value_counts().plot(figsize=(15, 8), kind='bar')\n",
    "plt.grid(True)\n",
    "plt.title('Emotions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'nocode' and multiple-emotion categories,\n",
    "df = df[df['emotions'] != 'nocode']\n",
    "df = df[~df['emotions'].str.contains('\\|')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>614484565059596288</th>\n",
       "      <td>Dorian Gray with Rainbow Scarf #LoveWins (from...</td>\n",
       "      <td>happy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614746522043973632</th>\n",
       "      <td>@SelectShowcase @Tate_StIves ... Replace with ...</td>\n",
       "      <td>happy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614877582664835073</th>\n",
       "      <td>@Sofabsports thank you for following me back. ...</td>\n",
       "      <td>happy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611932373039644672</th>\n",
       "      <td>@britishmuseum @TudorHistory What a beautiful ...</td>\n",
       "      <td>happy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611570404268883969</th>\n",
       "      <td>@NationalGallery @ThePoldarkian I have always ...</td>\n",
       "      <td>happy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 text  \\\n",
       "tweetid                                                                 \n",
       "614484565059596288  Dorian Gray with Rainbow Scarf #LoveWins (from...   \n",
       "614746522043973632  @SelectShowcase @Tate_StIves ... Replace with ...   \n",
       "614877582664835073  @Sofabsports thank you for following me back. ...   \n",
       "611932373039644672  @britishmuseum @TudorHistory What a beautiful ...   \n",
       "611570404268883969  @NationalGallery @ThePoldarkian I have always ...   \n",
       "\n",
       "                   emotions  category  \n",
       "tweetid                                \n",
       "614484565059596288    happy         0  \n",
       "614746522043973632    happy         0  \n",
       "614877582664835073    happy         0  \n",
       "611932373039644672    happy         0  \n",
       "611570404268883969    happy         0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'] = pd.factorize(df['emotions'])[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN/VALIDATION SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df.text,\n",
    "    df.category,\n",
    "    test_size=0.15,\n",
    "    random_state=17,\n",
    "    stratify=df.category.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotions</th>\n",
       "      <th>category</th>\n",
       "      <th>data_split</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">angry</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">disgust</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>train</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">happy</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>966</td>\n",
       "      <td>966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>171</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">not-relevant</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">sad</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>train</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">surprise</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>train</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  text  data_type\n",
       "emotions     category data_split                 \n",
       "angry        2        train         48         48\n",
       "                      val            9          9\n",
       "disgust      3        train          5          5\n",
       "                      val            1          1\n",
       "happy        0        train        966        966\n",
       "                      val          171        171\n",
       "not-relevant 1        train        182        182\n",
       "                      val           32         32\n",
       "sad          4        train         27         27\n",
       "                      val            5          5\n",
       "surprise     5        train         30         30\n",
       "                      val            5          5"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['data_type'] = ['NA'] * len(df)\n",
    "df.loc[X_train.index, 'data_split'] = 'train'\n",
    "df.loc[X_val.index, 'data_split'] = 'val'\n",
    "\n",
    "df.groupby(['emotions', 'category', 'data_split']).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize/encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/naziko/opt/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n",
      "Keyword arguments {'return_attention': True} not recognized.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
    "                                         do_lower_case=True)\n",
    "\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df[df['data_split'] == 'train']['text'].values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=256,\n",
    "    return_tensors='pt')\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df[df['data_split'] == 'val']['text'].values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention=True,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=256,\n",
    "    return_tensors='pt')\n",
    "\n",
    "indices_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "cats_train = torch.tensor(df[df['data_split'] == 'train']['category'].values)\n",
    "\n",
    "indices_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "cats_val = torch.tensor(df[df['data_split'] == 'val']['category'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = TensorDataset(indices_train,\n",
    "                        attention_masks_train,\n",
    "                        cats_train)\n",
    "df_val = TensorDataset(indices_val,\n",
    "                      attention_masks_val,\n",
    "                      cats_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1258, 223)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=len(df['category'].unique()),\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "dl_train = DataLoader(df_train,\n",
    "                      sampler=RandomSampler(df_train),\n",
    "                      batch_size=batch_size)\n",
    "\n",
    "dl_val = DataLoader(df_val,\n",
    "                    sampler=RandomSampler(df_val),\n",
    "                    batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                 lr=1e-5,\n",
    "                 eps=1e-8)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=len(dl_train)*epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(pred, category):\n",
    "    pred_flat = np.argmax(pred, axis=1).flatten()\n",
    "    category_flat = category.flatten()\n",
    "    \n",
    "    return f1_score(category_flat, pred_flat, average='weighted')\n",
    "\n",
    "cat_dict_inverse = {}\n",
    "    \n",
    "for cat in df['category'].unique():\n",
    "    cat_dict_inverse[cat] = df[df['category'] == cat]['emotions'].iloc[0]\n",
    "        \n",
    "        \n",
    "def accuracy_per_class(pred, category):\n",
    "    \n",
    "    \n",
    "    pred_flat = np.argmax(pred, axis=1).flatten()\n",
    "    category_flat = category.flatten()\n",
    "    \n",
    "    for cat in np.unique(category_flat):\n",
    "        y_pred = pred_flat[category_flat == cat]\n",
    "        y_true = category_flat[category_flat == cat]\n",
    "        \n",
    "        print(f'Class: {cat_dict_inverse[cat]}')\n",
    "        print(f'Accuracy: {len(y_pred[y_pred == cat])}/{len(y_true)}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed = 17\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dl_val):\n",
    "    \n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dl_val:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2]}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        category_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(category_ids)\n",
    "        \n",
    "    loss_val_avg = loss_val_total/len(dl_val)\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "    \n",
    "    return loss_val_avg, predictions, true_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b684de8ceb4cd58cbcf473d8f027ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 1', max=315.0, style=ProgressStyle(description_widt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 2', max=315.0, style=ProgressStyle(description_widt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 3', max=315.0, style=ProgressStyle(description_widt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 4', max=315.0, style=ProgressStyle(description_widt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 5', max=315.0, style=ProgressStyle(description_widt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 6', max=315.0, style=ProgressStyle(description_widt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 7', max=315.0, style=ProgressStyle(description_widt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 8', max=315.0, style=ProgressStyle(description_widt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 9', max=315.0, style=ProgressStyle(description_widt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cf27fd2b724299a94177348f68efdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 10', max=315.0, style=ProgressStyle(description_wid"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "    \n",
    "    progress_bar = tqdm(dl_train,\n",
    "                       desc='Epoch {:1d}'.format(epoch),\n",
    "                       leave=False,\n",
    "                       disable=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        model.zero_grad()\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2]}\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch {epoch}\n",
      "Training loss: 0.018273504548484372\n",
      "Valdation loss: 0.7002162379877908\n",
      "F1 Score (weighted): 0.8643009149471824\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), f'BERT_ft_epoch{epoch}.model')\n",
    "\n",
    "tqdm.write('\\nEpoch {epoch}')\n",
    "\n",
    "loss_train_avg = loss_train_total/len(dl_train)\n",
    "tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "\n",
    "val_loss, predictions, true_vals = evaluate(dl_val)\n",
    "val_f1 = f1_score_func(predictions, true_vals)\n",
    "tqdm.write(f'Valdation loss: {val_loss}')\n",
    "tqdm.write(f'F1 Score (weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "                                                     num_labels=len(cat_dict_inverse),\n",
    "                                                     output_attentions=False,\n",
    "                                                     output_hidden_states=False)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: happy\n",
      "Accuracy: 164/171\n",
      "\n",
      "Class: not-relevant\n",
      "Accuracy: 19/32\n",
      "\n",
      "Class: angry\n",
      "Accuracy: 7/9\n",
      "\n",
      "Class: disgust\n",
      "Accuracy: 0/1\n",
      "\n",
      "Class: sad\n",
      "Accuracy: 2/5\n",
      "\n",
      "Class: surprise\n",
      "Accuracy: 2/5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('BERT_ft_epoch10.model',\n",
    "                                map_location=torch.device('cpu')))\n",
    "\n",
    "_, predictions,true_vals = evaluate(dl_val)\n",
    "\n",
    "accuracy_per_class(predictions, true_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
